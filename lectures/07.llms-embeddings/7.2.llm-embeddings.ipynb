{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b146c0ef-83b3-4b16-998e-2fd737869315",
   "metadata": {},
   "source": [
    "# Large Language Models (LLMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d5f7f-29fa-4602-8381-8dedcf77d7ed",
   "metadata": {},
   "source": [
    "[ollama.com](https://ollama.com)  \n",
    "[ollama Github doc](https://github.com/ollama/ollama)  \n",
    "[ollama Python doc](https://github.com/ollama/ollama-python)  \n",
    "[markdown doc](https://python-markdown.github.io/reference/)\n",
    "\n",
    "Large Language Models (LLMs) only emerged in the second decade of the 21<sup>st</sup> century, yet have taken the world by storm. At their core, they are **classifiers**: they give you the probabilities of all possible next **tokens** (= words, part of words, or letters, the *vocabulary* of the model) given a context/prefix/prompt (the preceding tokens).\n",
    "\n",
    "If you want to know more, I highly recommend [this (technical, but very well illustrated) series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) on the subject.\n",
    "\n",
    "We will be using the library [Ollama](https://ollama.com) ([github](https://github.com/ollama/ollama) – itself based on a lower-level project called [llama.cpp](https://github.com/ggml-org/llama.cpp)), to run LLMs. locally. The other place I recommend looking up if you want to know more is [Huggingface](https://huggingface.co/), which is a set of libraries, a hub to share models and datasets, and a provider of many tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12567bf4-2968-40d3-9dcc-b24b879808a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import IPython\n",
    "import numpy as np\n",
    "\n",
    "import markdown\n",
    "import strip_markdown\n",
    "\n",
    "import base64\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6074bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to check if a model is available: if not: download it\n",
    "def check_model_and_pull(m_name):\n",
    "    # test if the model is downloaded, if not pull (download) from the server\n",
    "    if m_name not in [m.model for m in ollama.list().models]:\n",
    "        print(f\"model '{m_name}' not found, downloading...\")\n",
    "        # pull/download model\n",
    "        ollama.pull(m_name)\n",
    "    else:\n",
    "        print(f\"model: `{m_name}` found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb34d93",
   "metadata": {},
   "source": [
    "[Gemma 3 family](https://ollama.com/library/gemma3)  \n",
    "[Gemma 3 270m](https://ollama.com/library/gemma3:270m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma3:270m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dd7ef-2c79-44a9-ab23-3cb0f50e9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if the model is downloaded, if not pull from the server\n",
    "check_model_and_pull(model_name)\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=model_name, \n",
    "    messages=[\n",
    "      { \"role\": \"user\", \"content\": \"Why is the sky blue?\" },\n",
    "    ]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358924dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2bd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"message\"][\"content\"])\n",
    "# you can also access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14359474",
   "metadata": {},
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df83415",
   "metadata": {},
   "source": [
    "Full list [here](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ef014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"temperature\": 0.,\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1d44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"temperature\": 10.,\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984fcecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"temperature\": 10,\n",
    "            \"num_predict\": 5\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86721c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ollama.chat(\n",
    "        model=model_name,\n",
    "        messages=[{ \"role\": \"user\", \"content\": \"Why is the sky blue?\" },],\n",
    "        options={\n",
    "            \"stop\": [\"a\"],\n",
    "        })[\"message\"][\"content\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9e9c20-783e-4e75-9056-fc1a2702d02c",
   "metadata": {},
   "source": [
    "## Note: handle markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0961d9",
   "metadata": {},
   "source": [
    "[Markdown](https://pypi.org/project/Markdown/)  \n",
    "[strip-markdown](https://pypi.org/project/strip-markdown/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73216dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6322cbb6-7e7e-422b-973c-404c262c33bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md2html(text):\n",
    "    return markdown.markdown(text)\n",
    "\n",
    "def print_html(raw_html):\n",
    "    IPython.display.display_html(raw_html, raw=True)\n",
    "\n",
    "print_html(md2html(response.message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda090e-79f7-472e-be09-a6eab2a52923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_md(text):\n",
    "    return strip_markdown.strip_markdown(text)\n",
    "\n",
    "print(strip_md(response.message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5096635-de00-4e34-922f-d3f51e0335c6",
   "metadata": {},
   "source": [
    "## Gradual printing / streaming responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a32f49-5e20-4aa3-8474-b9bae9e5e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04775a-5f39-4664-b239-b37ff24ecfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.chat(\n",
    "    model=model_name,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "text = \"\"\n",
    "for chunk in stream:\n",
    "    text += chunk[\"message\"][\"content\"]\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    print_html(md2html(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0aa30",
   "metadata": {},
   "source": [
    "### Extra: Generate experiment: removing the template!\n",
    "\n",
    "[doc](https://github.com/ollama/ollama-python?tab=readme-ov-file#generate), [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c50727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Hi, my name is Jeremie.\"\n",
    "\n",
    "print(\n",
    "    ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt=prompt,\n",
    "        # if `True`` no formatting will be applied to the prompt!\n",
    "        raw=True,\n",
    "        options= {\n",
    "            # here we limit the output to 50 tokens only\n",
    "            \"num_predict\": 50,\n",
    "            # you can play with the temperature if you want\n",
    "            # \"temperature\": .9,\n",
    "            }\n",
    "        )[\"response\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9bafe",
   "metadata": {},
   "source": [
    "Looking at the [template](https://ollama.com/library/gemma3:270m/blobs/4b19ac7dd2fb) (also [test it live in Tiktokenizer!](https://tiktokenizer.vercel.app/?model=google%2Fgemma-7b), an earlier model in this series but the template looks the same), I can manually recreate the text that the model actually reads, so that it behaves again like a chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the structure has these special markers + new lines\n",
    "prompt = \"\"\"<start_of_turn>user\n",
    "Hi, my name is Jeremie.<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "# if you use this, the model will actually start answering as the *user*!\n",
    "# prompt = \"\"\"<start_of_turn>user\n",
    "# Hi, my name is Jeremie.<end_of_turn>\n",
    "# <start_of_turn>model\n",
    "# Hi, I'm a weird machine, how can I be of help?\n",
    "# <start_of_turn>user\n",
    "# \"\"\"\n",
    "\n",
    "print(\n",
    "    ollama.generate(\n",
    "        model=model_name,\n",
    "        prompt= prompt,\n",
    "        # if `True`` no formatting will be applied to the prompt!\n",
    "        raw=True,\n",
    "        options= {\n",
    "            # here we limit the output to 50 tokens only\n",
    "            \"num_predict\": 50,\n",
    "            # \"temperature\": .9,\n",
    "            \n",
    "            }\n",
    "        )[\"response\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5b033-1ef8-4643-a726-07d7551eafe3",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25eae66",
   "metadata": {},
   "source": [
    "Recent models allow you to work with the vector representation of their input, aka **embeddings**! Not all models allow you to do this, so you need to check their model card on Ollama. Here I use [all-minilm](https://ollama.com/library/all-minilm) ([Ollama search](https://ollama.com/search) includes tags (like `embedding`), giving you the models that support that.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc723c1-ed11-4d53-870d-9d09d6ff4de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_name = \"all-minilm\"\n",
    "\n",
    "# test if the model is downloaded, if not pull from the server\n",
    "if embed_model_name not in [m.model for m in ollama.list().models]:\n",
    "    ollama.pull(embed_model_name)\n",
    "    \n",
    "response = ollama.embed(\n",
    "    model=embed_model_name,\n",
    "    input=[\"Why is the sky blue?\"], # can be a single string, or a list of strings\n",
    ")\n",
    "\n",
    "# or access fields directly from the response object\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e46f2ef-5506-43e5-bc9e-daa486db2948",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b789ef-b117-44fd-852b-3d73617e06b5",
   "metadata": {},
   "source": [
    "[wiki](https://en.wikipedia.org/wiki/Cosine_similarity#Definition)  \n",
    "\n",
    "We want to be able to measure how similar two vectors are. One way of doing this is to use trigonometry to compute the **angle** between them:\n",
    "\n",
    "$$ \\cos(\\theta) = \\frac{\\overbrace{ A \\cdot B }^{\\text{dot product of } A \\text{ and } B } }{\\underbrace{ \\|A\\| \\|B\\| }_{ \\substack{ \\text{normalize to 1} \\\\ \\text{ (vector length has no impact) } } } }$$\n",
    "\n",
    "The \"$\\cdot$\" is the [dot product](https://www.youtube.com/watch?v=LyGKycYT2v0&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=9) (multiplying all elements then adding the results together: if $A = (a_1, a_2, \\dots, a_n), B = (b_1, b_2, \\dots, b_n)$, then $A \\cdot B = a_1 b_1 + a_2 b_2 + \\dots + a_n b_n$). The division by $\\|A\\| \\|B\\|$, the normalisation, can be read as us making sure we're working with the unit circle (where we can apply trigonometry)!\n",
    "\n",
    "![cosine similarity images](pics/cosine-similarity-vectors.jpg)\n",
    "[source](https://www.learndatasci.com/glossary/cosine-similarity/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266c383-ba63-4e9b-958b-b98517eaa909",
   "metadata": {},
   "source": [
    "#### Maths Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff461923-d971-4672-bfd4-ed27b131d725",
   "metadata": {},
   "source": [
    "Why is $\\cos(\\theta)$ close to 1 when $\\theta$ is close to zero? On the unit circle – circle with radius of 1, which is what we have since we normalised through the division by $\\|A\\| \\|B\\|$ – , $\\cos(\\theta)$ and $\\sin(\\theta)$ are in fact the $x,y$ coordinate of the point on the circle intersecting the vector with that angle! (Or $\\cos(\\theta) = \\frac{ \\text{adj} }{ \\text{hyp}\\ (= 1) } = \\text{adj}$).\n",
    "\n",
    "![sin cos tan unit circle figure](pics/unit-circle-sin-cos-tan.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e22348f-48ce-42db-9be3-4329778e17d7",
   "metadata": {},
   "source": [
    "#### Code Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ae97b-27b2-4bed-be9b-00bc81d43452",
   "metadata": {},
   "source": [
    "You can also find it implemented in `scikit-learn`, you can add it with `mamba` and then import it:\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb00e0-805e-400c-ac85-3fcdf5f6c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    See here: https://gist.github.com/robert-mcdermott/5957ef1ddcfc7c3ba898d800531b2aa7\n",
    "    \"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    cosine_similarity = dot_product / (norm1 * norm2)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a578e5",
   "metadata": {},
   "source": [
    "### Comparing sentence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4938b-1349-45ed-b040-bb68d03e2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Why is the sky blue?\",\n",
    "    \"Why is the sky orange?\",\n",
    "    \"Tonight I'll be eating soup.\"\n",
    "]\n",
    "\n",
    "response = ollama.embed(\n",
    "    model=embed_model_name,\n",
    "    input=sentences,\n",
    ")\n",
    "\n",
    "# or access fields directly from the response object\n",
    "print(len(response.embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8cacd9-6f52-4261-845e-c5974f16b49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_similarities(s1_id, s2_id, sentences, embeddings):\n",
    "    print(f\"Similarity between:\")\n",
    "    print(f\" - '{sentences[s1_id]}'\")\n",
    "    print(f\" - '{sentences[s2_id]}'\")\n",
    "    print(f\"   => {cosine_similarity(embeddings[s1_id], embeddings[s2_id])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593a8d6-a94a-4cee-8a7a-0c616cf766d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_similarities(0, 1, sentences, response.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39134817-a244-4369-95b3-2fe79ba3f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_similarities(0, 2, sentences, response.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bc3d7-eea5-4cb0-834c-0e82e16b6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_similarities(1, 2, sentences, response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba4a5df",
   "metadata": {},
   "source": [
    "## Extra: multimodality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a1e8b",
   "metadata": {},
   "source": [
    "[Gemma 3, 4b doc](https://ollama.com/library/gemma3:4b)\n",
    "\n",
    "**Multimodality** refers to the fact that models are trained, and therefore can understand/interact with, multiple types of data: in the most common case, it's *text* and *images* (could also be sound, video, sensory data from a robot, etc.). Here you can see an example of a model that is able to read/understand images.\n",
    "\n",
    "Adapted from the [gemma3 example](https://github.com/ollama/ollama-python/blob/main/examples/multimodal-chat.py) – a bigger model, taking around 2-3GB in RAM. (See also the [llava example](https://github.com/ollama/ollama-python/blob/main/examples/multimodal-generate.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our new model name \n",
    "model_name = \"gemma3:4b\"\n",
    "\n",
    "# download it if not present\n",
    "check_model_and_pull(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38acf331",
   "metadata": {},
   "source": [
    "[base64 doc](https://docs.python.org/3/library/base64.html)  \n",
    "[w<sup>3</sup> tutorial](https://www.w3schools.com/Python/ref_module_base64.asp)  \n",
    "[base64 RealPython tutorial](https://realpython.com/python-serialize-data/)\n",
    "\n",
    "Ollama supports feeding the image as a path, a `base64` string (a kind of encoding, using only ASCII characters), or raw bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c8bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_option = 0\n",
    "\n",
    "# pass in the path to the image\n",
    "path_or_img = \"pics/cosine-similarity-vectors.jpg\"\n",
    "\n",
    "if file_option == 1:\n",
    "    # you can also pass in base64 encoded image data\n",
    "    path_or_img = base64.b64encode(Path(path_or_img).read_bytes()).decode()\n",
    "elif file_option == 2:\n",
    "    # or the raw bytes\n",
    "    path_or_img = Path(path_or_img).read_bytes()\n",
    "\n",
    "response = ollama.chat(\n",
    "    # note that \n",
    "    model=model_name,\n",
    "    messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': 'What is in this image? Be concise.',\n",
    "            # we just add an 'image' key:value pair to our message object\n",
    "            # the value is a list, containing one or more images (as paths/base64 str/bytes)\n",
    "            'images': [path_or_img],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5744b62f",
   "metadata": {},
   "source": [
    "### Extra: wanna see the `base64` string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79865fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read raw bytes, then encode as b64 (still bytes), then decode as string\n",
    "image_as_str = base64.b64encode(Path(path_or_img).read_bytes()).decode()\n",
    "# only the first 100 characters\n",
    "image_as_str[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9d712",
   "metadata": {},
   "source": [
    "## Extra: tools, web browsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f89e04",
   "metadata": {},
   "source": [
    "Recent models have even more functionalities, such as:\n",
    "- using tools/calling functions ([tools example](https://github.com/ollama/ollama-python/blob/main/examples/tools.py), [multi-tool example](https://github.com/ollama/ollama-python/blob/main/examples/multi-tool.py), [async tools](https://github.com/ollama/ollama-python/blob/main/examples/async-tools.py))\n",
    "- 'thinking' (namely generate more tokens to arrive at an answer) ([thinking chat example](https://github.com/ollama/ollama-python/blob/main/examples/thinking.py), [thinking generate example](https://github.com/ollama/ollama-python/blob/main/examples/thinking-generate.py), [thinking levels examples](https://github.com/ollama/ollama-python/blob/main/examples/thinking-levels.py))\n",
    "- web-browsing ([qwen example](https://github.com/ollama/ollama-python/blob/main/examples/web-search.py), [gpt-oss example](https://github.com/ollama/ollama-python/blob/main/examples/web-search-gpt-oss.py))\n",
    "\n",
    "\n",
    "Beware, many recent models require a lot of memory. For example **gpt-oss-20b** requires just under 12GB of RAM..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
